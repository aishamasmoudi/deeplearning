{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HITId                                              tweet       sentiment  \\\n",
      "0      1  صلاة الفجر خير لك من ترديد بول البعير وسبي الن...  hateful_normal   \n",
      "1      2  صراحة نفسي اشوف ولاد الوسخة اللي قالوا مدرب اج...       offensive   \n",
      "2      3  طيب! هي متبرجة وعبايتها ملونه وطالعة من بيتهم ...       offensive   \n",
      "3      4  @user @user انا اوافقك بخصوص السوريين و العراق...          normal   \n",
      "4      5  هذه السعودية التي شعبها شعب الخيم و بول البعير...          normal   \n",
      "\n",
      "  directness                           annotator_sentiment  target       group  \n",
      "0   indirect                                         shock  gender  individual  \n",
      "1   indirect  anger_confusion_sadness_indifference_disgust   other       other  \n",
      "2   indirect                                  indifference   other  individual  \n",
      "3     direct                                  indifference  origin       other  \n",
      "4   indirect                                  indifference  origin       other  \n",
      "(18661, 7)\n"
     ]
    }
   ],
   "source": [
    "#dowlnoad the dataset\n",
    "data_load = load_dataset(\"nedjmaou/MLMA_hate_speech\")\n",
    "data_train = data_load[\"train\"]\n",
    "dataset = data_train.to_pandas()\n",
    "print(dataset.head())\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 8)\n",
      "   HITId                                              tweet sentiment  \\\n",
      "0   1486                          بول البعير وعلاج السرطان!    normal   \n",
      "1   3194  الإسلام يتعامل مع القضايا قبل حصولها فيسعى لعل...    normal   \n",
      "2   1384  @user @user معلش سؤال..الكائن الابيض دا نوعه ا...    normal   \n",
      "3   1391  محاضرة التحرش والابتزاز كلية الحقوق والعلوم ال...    normal   \n",
      "4   1420  حرب شرسه على المرآه وحجابها من كل مكان الاعلام...    normal   \n",
      "\n",
      "  directness   annotator_sentiment    target  group  sentiment_binary  \n",
      "0   indirect  indifference_disgust     other  other                 1  \n",
      "1   indirect          indifference  religion  other                 1  \n",
      "2   indirect                 shock    origin  other                 1  \n",
      "3   indirect          indifference     other  other                 1  \n",
      "4     direct               sadness     other  women                 1  \n",
      "(600, 8)\n",
      "   HITId                                              tweet sentiment  \\\n",
      "0   5421                                         bad nigger    normal   \n",
      "1   2605  drop hard might next negro spiritual last new ...    normal   \n",
      "2   1269                      maybe called spic today  @url    normal   \n",
      "3   4375  -sighed. please stop saying ching chong. reall...    normal   \n",
      "4    935  les #migrants traversent la mer et trouvent le...    normal   \n",
      "\n",
      "  directness                         annotator_sentiment  target  \\\n",
      "0   indirect  anger_confusion_shock_indifference_disgust   other   \n",
      "1   indirect                               shock_disgust  origin   \n",
      "2   indirect                        indifference_disgust  origin   \n",
      "3   indirect                              confusion_fear  origin   \n",
      "4   indirect                               anger_sadness  origin   \n",
      "\n",
      "           group  sentiment_binary  \n",
      "0          other                 1  \n",
      "1  special_needs                 1  \n",
      "2       refugees                 1  \n",
      "3        muslims                 1  \n",
      "4          other                 1  \n",
      "(600, 8)\n",
      "   HITId                                              tweet sentiment  \\\n",
      "0    437  @user du coup pour l’attarde c’est celui qui é...    normal   \n",
      "1     64  @user @user @user mr simon vous ne pouvez just...    normal   \n",
      "2    125  les tutos en français sur youtube c’est insupp...    normal   \n",
      "3    750  @user pq y'a autant de rebeus qui tiennent ave...    normal   \n",
      "4    599  @user non à la dictature de la pensée non au t...    normal   \n",
      "\n",
      "  directness   annotator_sentiment  target          group  sentiment_binary  \n",
      "0     direct          indifference  origin  special_needs                 1  \n",
      "1   indirect          indifference   other          other                 1  \n",
      "2     direct                 shock  origin          other                 1  \n",
      "3   indirect  sadness_indifference  origin          arabs                 1  \n",
      "4   indirect               sadness   other          other                 1  \n"
     ]
    }
   ],
   "source": [
    "#dataset separate by languages\n",
    "arab_path = \"/Users/benoitmathey-doret/Documents/EPFL/MA2/DL/project/deeplearning/dataset/ar_dataset_600.csv\"\n",
    "ar_dataset = pd.read_csv(arab_path)\n",
    "print(ar_dataset.shape)\n",
    "print(ar_dataset.head())\n",
    "ar_dataset = ar_dataset.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "en_path=\"/Users/benoitmathey-doret/Documents/EPFL/MA2/DL/project/deeplearning/dataset/en_dataset_600.csv\"\n",
    "en_dataset = pd.read_csv(en_path)\n",
    "print(en_dataset.shape)\n",
    "print(en_dataset.head())\n",
    "en_dataset = en_dataset.sample(frac=1, random_state=42)\n",
    "\n",
    "en_dataset_binary=en_dataset.copy()\n",
    "en_dataset_binary[\"sentiment_binary\"]=en_dataset_binary[\"sentiment_binary\"].replace(3,2)\n",
    "\n",
    "fr_path=\"/Users/benoitmathey-doret/Documents/EPFL/MA2/DL/project/deeplearning/dataset/fr_dataset_600.csv\"\n",
    "fr_dataset = pd.read_csv(fr_path)\n",
    "print(fr_dataset.shape)\n",
    "print(fr_dataset.head())\n",
    "fr_dataset = fr_dataset.sample(frac=1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:28<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/gm/941r48zd5qxcl4tb3v9q30440000gn/T/ipykernel_93745/3750342181.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  used_data['predictions'] = predictions_concat\n"
     ]
    }
   ],
   "source": [
    "#One by One\n",
    "\n",
    "used_data = en_dataset_binary.iloc[0:40]\n",
    "predictions_concat=[]\n",
    "\n",
    "# Wrap range with tqdm\n",
    "for i in tqdm(range(used_data.shape[0])):\n",
    "    tweet = en_dataset['tweet'].iloc[i]\n",
    "    # prompt\n",
    "    prompt = [{\"role\": \"system\", \"content\": \"Do you think these tweets are hate speech or normal speech. Try your best even if we do not have context. Display the reasult as an integer of 1 (Normal) or 2 (Hateful). Do not display anything else, it is important that only the interger is displayed.\"},\n",
    "              #{\"role\": \"system\", \"content\": \"Do you think these tweets are hate speech, offensive or normal speech. Try your best even if we do not have context. Display the reasult as an integer of 1 (Normal), 2(offensive) or 3 (Hateful). Do not display anything else, it is important that only the interger is displayed.\"},\n",
    "              {\"role\": \"user\", \"content\": tweet},]\n",
    "    # LLM initialization\n",
    "    llm = Ollama(model=\"llama3\")\n",
    "    predictions = llm.invoke(prompt)\n",
    "    predictions = int(predictions) \n",
    "    predictions_concat.append(predictions)\n",
    "\n",
    "print(len(predictions_concat))\n",
    "#add the predictions to the dataset\n",
    "used_data['predictions'] = predictions_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.775 F1:  0.7565922920892495 Precision:  0.8166144200626959 Recall:  0.7550505050505051\n"
     ]
    }
   ],
   "source": [
    "#Metrics\n",
    "acc=accuracy_score(used_data['sentiment_binary'], used_data['predictions'])\n",
    "F1=f1_score(used_data['sentiment_binary'], used_data['predictions'], average='macro')\n",
    "Precision=precision_score(used_data['sentiment_binary'], used_data['predictions'], average='macro')\n",
    "Recall=recall_score(used_data['sentiment_binary'], used_data['predictions'], average='macro')\n",
    "print(\"Accuracy: \", acc, \"F1: \", F1, \"Precision: \", Precision, \"Recall: \", Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sentiment  sentiment_binary  predictions\n",
      "110     normal                 1            1\n",
      "419    hateful                 2            2\n",
      "565    hateful                 2            2\n",
      "77      normal                 1            1\n",
      "181     normal                 1            2\n",
      "284  offensive                 2            2\n",
      "10      normal                 1            2\n",
      "469    hateful                 2            2\n",
      "78      normal                 1            1\n",
      "349  offensive                 2            2\n",
      "55      normal                 1            2\n",
      "118     normal                 1            2\n",
      "109     normal                 1            1\n",
      "588    hateful                 2            2\n",
      "369  offensive                 2            2\n",
      "234  offensive                 2            2\n",
      "30      normal                 1            1\n",
      "212  offensive                 2            1\n",
      "184     normal                 1            2\n",
      "86      normal                 1            2\n",
      "2       normal                 1            2\n",
      "587    hateful                 2            2\n",
      "535    hateful                 2            2\n",
      "596    hateful                 2            2\n",
      "368  offensive                 2            2\n",
      "539    hateful                 2            2\n",
      "72      normal                 1            2\n",
      "135     normal                 1            1\n",
      "556    hateful                 2            2\n",
      "437    hateful                 2            2\n",
      "595    hateful                 2            2\n",
      "163     normal                 1            1\n",
      "82      normal                 1            1\n",
      "257  offensive                 2            2\n",
      "192     normal                 1            1\n",
      "453    hateful                 2            2\n",
      "132     normal                 1            1\n",
      "515    hateful                 2            2\n",
      "441    hateful                 2            2\n",
      "275  offensive                 2            2\n"
     ]
    }
   ],
   "source": [
    "print(used_data[[ 'sentiment','sentiment_binary', 'predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 2]\n"
     ]
    }
   ],
   "source": [
    "print(en_dataset['sentiment_binary'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
