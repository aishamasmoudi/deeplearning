{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import re\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HITId                                              tweet       sentiment  \\\n",
      "0      1  صلاة الفجر خير لك من ترديد بول البعير وسبي الن...  hateful_normal   \n",
      "1      2  صراحة نفسي اشوف ولاد الوسخة اللي قالوا مدرب اج...       offensive   \n",
      "2      3  طيب! هي متبرجة وعبايتها ملونه وطالعة من بيتهم ...       offensive   \n",
      "3      4  @user @user انا اوافقك بخصوص السوريين و العراق...          normal   \n",
      "4      5  هذه السعودية التي شعبها شعب الخيم و بول البعير...          normal   \n",
      "\n",
      "  directness                           annotator_sentiment  target       group  \n",
      "0   indirect                                         shock  gender  individual  \n",
      "1   indirect  anger_confusion_sadness_indifference_disgust   other       other  \n",
      "2   indirect                                  indifference   other  individual  \n",
      "3     direct                                  indifference  origin       other  \n",
      "4   indirect                                  indifference  origin       other  \n",
      "(18661, 7)\n"
     ]
    }
   ],
   "source": [
    "#dowlnoad the dataset\n",
    "data_load = load_dataset(\"nedjmaou/MLMA_hate_speech\")\n",
    "data_train = data_load[\"train\"]\n",
    "dataset = data_train.to_pandas()\n",
    "print(dataset.head())\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset separate by languages\n",
    "arab_path = \"C:/Users/masmoudi/deeplearning/dataset/ar_dataset_600.csv\"\n",
    "ar_dataset = pd.read_csv(arab_path)\n",
    "#print(ar_dataset.shape)\n",
    "#print(ar_dataset.head())\n",
    "ar_dataset = ar_dataset.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "en_path=\"C:/Users/masmoudi/deeplearning/dataset/en_dataset_600.csv\"\n",
    "en_dataset = pd.read_csv(en_path)\n",
    "#print(en_dataset.shape)\n",
    "#print(en_dataset.head())\n",
    "en_dataset = en_dataset.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "fr_path=\"C:/Users/masmoudi/deeplearning/dataset/fr_dataset_600.csv\"\n",
    "fr_dataset = pd.read_csv(fr_path)\n",
    "#print(fr_dataset.shape)\n",
    "#print(fr_dataset.head())\n",
    "fr_dataset = fr_dataset.sample(frac=1, random_state=42)\n",
    "\n",
    "#create binary datasets\n",
    "en_dataset_binary=en_dataset.copy()\n",
    "en_dataset_binary = en_dataset_binary[en_dataset_binary['sentiment_binary'].isin([1, 3])]\n",
    "en_dataset_binary['sentiment_binary'] = en_dataset_binary['sentiment_binary'].replace(2, 0)\n",
    "en_dataset_binary['sentiment_binary'] = en_dataset_binary['sentiment_binary'].replace(3, 1)\n",
    "\n",
    "ar_dataset_binary=ar_dataset.copy()\n",
    "ar_dataset_binary = ar_dataset_binary[ar_dataset_binary['sentiment_binary'].isin([1, 3])]\n",
    "ar_dataset_binary['sentiment_binary'] = ar_dataset_binary['sentiment_binary'].replace(2, 0)\n",
    "ar_dataset_binary['sentiment_binary'] = ar_dataset_binary['sentiment_binary'].replace(3, 1)\n",
    "\n",
    "fr_dataset_binary=fr_dataset.copy()\n",
    "fr_dataset_binary = fr_dataset_binary[fr_dataset_binary['sentiment_binary'].isin([1, 3])]\n",
    "fr_dataset_binary['sentiment_binary'] = fr_dataset_binary['sentiment_binary'].replace(2, 0)\n",
    "fr_dataset_binary['sentiment_binary'] = fr_dataset_binary['sentiment_binary'].replace(3, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 9)\n",
      "   Unnamed: 0  HITId sentiment directness   annotator_sentiment    target  \\\n",
      "0           0   1486    normal   indirect  indifference_disgust     other   \n",
      "1           1   3194    normal   indirect          indifference  religion   \n",
      "2           2   1384    normal   indirect                 shock    origin   \n",
      "3           3   1391    normal   indirect          indifference     other   \n",
      "4           4   1420    normal     direct               sadness     other   \n",
      "\n",
      "   group  sentiment_binary                                              tweet  \n",
      "0  other                 1  The urine of the camel and the treatment of ca...  \n",
      "1  other                 1  Islam deals with issues before it occurs, and ...  \n",
      "2  other                 1  @user @user is not a question .. white object ...  \n",
      "3  other                 1  Lecture of Harassment and extortion, Faculty o...  \n",
      "4  women                 1  A fierce war on the mirror and its veil from e...  \n",
      "(600, 9)\n",
      "   Unnamed: 0  HITId sentiment directness   annotator_sentiment  target  \\\n",
      "0           0    437    normal     direct          indifference  origin   \n",
      "1           1     64    normal   indirect          indifference   other   \n",
      "2           2    125    normal     direct                 shock  origin   \n",
      "3           3    750    normal   indirect  sadness_indifference  origin   \n",
      "4           4    599    normal   indirect               sadness   other   \n",
      "\n",
      "           group  sentiment_binary  \\\n",
      "0  special_needs                 1   \n",
      "1          other                 1   \n",
      "2          other                 1   \n",
      "3          arabs                 1   \n",
      "4          other                 1   \n",
      "\n",
      "                                               tweet  \n",
      "0  @use suddenly for the linger is the one who is...  \n",
      "1  @USER @USER @USER Mr Simon You cannot justify ...  \n",
      "2  The tutorials in French on Youtube are unbeara...  \n",
      "3      @User PQ is so many rebeus that hold with OL?  \n",
      "4  @user no to the dictatorship of thought not to...  \n"
     ]
    }
   ],
   "source": [
    "#dataset of arabic and french tweets translated to english\n",
    "arab_tr_path = \"C:/Users/masmoudi/deeplearning/dataset/ar_dataset_600_translated.csv\"\n",
    "ar_tr_dataset = pd.read_csv(arab_tr_path)\n",
    "print(ar_tr_dataset.shape)\n",
    "print(ar_tr_dataset.head())\n",
    "ar_tr_dataset = ar_tr_dataset.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "fr_tr_path=\"C:/Users/masmoudi/deeplearning/dataset/fr_dataset_600_translated.csv\"\n",
    "fr_tr_dataset = pd.read_csv(fr_tr_path)\n",
    "print(fr_tr_dataset.shape)\n",
    "print(fr_tr_dataset.head())\n",
    "fr_tr_dataset = fr_tr_dataset.sample(frac=1, random_state=42)\n",
    "\n",
    "#create binary datasets\n",
    "ar_tr_dataset_binary=ar_tr_dataset.copy()\n",
    "ar_tr_dataset_binary = ar_tr_dataset_binary[ar_tr_dataset_binary['sentiment_binary'].isin([1, 3])]\n",
    "ar_tr_dataset_binary['sentiment_binary'] = ar_tr_dataset_binary['sentiment_binary'].replace(2, 0)\n",
    "ar_tr_dataset_binary['sentiment_binary'] = ar_tr_dataset_binary['sentiment_binary'].replace(3, 1)\n",
    "\n",
    "fr_tr_dataset_binary=fr_tr_dataset.copy()\n",
    "fr_tr_dataset_binary = fr_tr_dataset_binary[fr_tr_dataset_binary['sentiment_binary'].isin([1, 3])]\n",
    "fr_tr_dataset_binary['sentiment_binary'] = fr_tr_dataset_binary['sentiment_binary'].replace(2, 0)\n",
    "fr_tr_dataset_binary['sentiment_binary'] = fr_tr_dataset_binary['sentiment_binary'].replace(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompts\n",
    "default_prompt = \"Is the following text hateful or not? Just answer in Yes or No. Don't provide explanations. Text:\"\n",
    "default_prompt_en_hate_speech_def = \"The definition of hate speech is ‘Hate speech’ is speech that attacks a person or group on the basis of attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity. Is the following text hateful or not? Just answer in Yes or No. Don't provide explanations. Text:\"\n",
    "default_prompt_fr_hate_speech_def = \"La définition du discours de haine est la suivante : le « discours de haine » est un discours qui attaque une personne ou un groupe en raison d’attributs tels que la race, la religion, l'origine ethnique, l'origine nationale, le sexe, le handicap, l'orientation sexuelle ou l'identité de genre. Is the following text hateful or not? Just answer in Yes or No. Don't provide explanations. Text:\"\n",
    "default_prompt_ar_hate_speech_def = \"تعريف خطاب الكراهية هو: 'خطاب الكراهية' هو خطاب يهاجم شخصًا أو مجموعة بناءً على سمات مثل العرق، الدين، الأصل العرقي، الأصل الوطني، الجنس، الإعاقة، التوجه الجنسي، أو الهوية الجندرية. Is the following text hateful or not? Just answer in Yes or No. Don't provide explanations. Text:\"\n",
    "default_prompt_hate_speech_def_3_languages = \"The definition of hate speech is ‘Hate speech’ is speech that attacks a person or group on the basis of attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity. تعريف خطاب الكراهية هو: 'خطاب الكراهية' هو خطاب يهاجم شخصًا أو مجموعة بناءً على سمات مثل العرق، الدين، الأصل العرقي، الأصل الوطني، الجنس، الإعاقة، التوجه الجنسي، أو الهوية الجندرية. La définition du discours de haine est la suivante : le « discours de haine » est un discours qui attaque une personne ou un groupe en raison d’attributs tels que la race, la religion, l'origine ethnique, l'origine nationale, le sexe, le handicap, l'orientation sexuelle ou l'identité de genre. Is the following text hateful or not? Just answer in Yes or No. Don't provide explanations. Text:\"\n",
    "few_shot_en_examples = \"Is the following text hateful or not? Just answer in Yes or No. Don't provide explanations. Example 1:  Text: 'i dont think im getting my baby them white 9 he has two white j and nikes not even touched'. Answer: No. Example 2: Text: 'we cannot continue calling ourselves feminists if the right of all womxn arent addressed yes to a sexual offences public list but will a trans lesbian bisexual and queer womxn be able to enter their information on the reporting sheet gender forum'. Answer: No. Example 3: Text: 'tariq nasheed beating the fuck out of that drum beloved lmao somebody put a biscuit in that niggas mouth already'. Answer: No. Example 3: Text: 'tariq nasheed beating the fuck out of that drum beloved lmao somebody put a biscuit in that niggas mouth already'. Answer: No. Example 4: Text: 'cotton thigh highs to avoid contact with any gay chemicals'. Answer: No. Example 5: Text: 'fellas be careful she texting other niggas'. Answer: No. Example 6: Text: 'laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate'. Answer: Yes. Example 7: Text: 'lol what a stupid kike'. Answer: Yes. Example 8: Text: 'this sounds like a kike national socialist'. Answer: Yes. Example 9: Text: 'just another dumb nigger'. Answer: Yes. Example 10: Text: 'they are going to figure out a way to kike bitcoin'. Answer: Yes. Text:\"\n",
    "few_shot_en_fr_ar_examples = \"Is the following text hateful or not? Just answer in Yes or No. Don't provide explanations. Example 1:  Text: 'i dont think im getting my baby them white 9 he has two white j and nikes not even touched'. Answer: No. Example 2: Text: 'we cannot continue calling ourselves feminists if the right of all womxn arent addressed yes to a sexual offences public list but will a trans lesbian bisexual and queer womxn be able to enter their information on the reporting sheet gender forum'. Answer: No. Example 3: Text: 'just another dumb nigger'. Answer: Yes. Example 4: Text: 'they are going to figure out a way to kike bitcoin'. Answer: Yes. Example 5: Text: 'يارب انصرنا #افريقيا_يا_اهلي'. Example 6: Text: 'الصور العارية واجسام البنات ومقاطع منحرفة ، ستشهد عليها الاصابع يوم القيامة ، كفاكم استهانة قالجنة اغلى'. Answer: No. Example 7: 'بس يا شارب بول البعير يا عبد ال سلول'. Answer: Yes. Example 8: Text: 'هههعععععع اي علوم ههههه بول البعير'. Answer: Yes. Example 9: Text: 'ohhh la violence je meuuurs'. Answer: No. Example 10: Text: 'pour ou contre l'avortement ?'. Answer: No. Example 11: Text: 'tu nous ramène une maladie de jérusalem?'. Anwer: Yes. Example 12: Text: 'le secours catholique ardent promoteur de l'ensauvagement et de l'islamisation de l'europe ! son catholicisme dévo…' Answer: Yes. Text:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 10/400 [00:22<14:46,  2.27s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Wrap range with tqdm\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(used_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m---> 13\u001b[0m     tweet \u001b[38;5;241m=\u001b[39m \u001b[43mused_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#default prompt\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     17\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: used_prompt},\n\u001b[0;32m     18\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: tweet}\n\u001b[0;32m     19\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\masmoudi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masmoudi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\masmoudi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "#run with specific prompt and different datasets\n",
    "#we run the following:\n",
    "#default prompt with en, fr, ar, translated fr and translated ar datasets\n",
    "#few-shot (only english examples) with en, fr and ar datasets\n",
    "#few-shot (examples in all languages) with en, fr and ar datasets\n",
    "used_dataset = en_dataset #define the dataset to use\n",
    "used_data = en_dataset_binary\n",
    "used_prompt = default_prompt #define the prompt used\n",
    "predictions_concat = []\n",
    "\n",
    "# Wrap range with tqdm\n",
    "for i in tqdm(range(used_data.shape[0])):\n",
    "    tweet = used_dataset['tweet'].iloc[i]\n",
    "\n",
    "    #default prompt\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": used_prompt},\n",
    "        {\"role\": \"user\", \"content\": tweet}\n",
    "    ]\n",
    "\n",
    "    # LLM initialization\n",
    "    llm = Ollama(model=\"llama3\")\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    if response.strip().lower() == 'yes':\n",
    "        predictions = 1\n",
    "    elif response.strip().lower() == 'no':\n",
    "        predictions = 0\n",
    "    else:\n",
    "        predictions = 0  # Default to non-hateful if the response is unexpected\n",
    "    \n",
    "    predictions_concat.append(predictions)\n",
    "\n",
    "print(len(predictions_concat))\n",
    "\n",
    "# Add the predictions to the dataset\n",
    "used_data['predictions'] = predictions_concat\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(used_data['sentiment_binary'], used_data['predictions'])\n",
    "F1 = f1_score(used_data['sentiment_binary'], used_data['predictions'], average='macro')\n",
    "Precision = precision_score(used_data['sentiment_binary'], used_data['predictions'], average='macro')\n",
    "Recall = recall_score(used_data['sentiment_binary'], used_data['predictions'], average='macro')\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f}, F1: {F1:.3f}, Precision: {Precision:.3f}, Recall: {Recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
